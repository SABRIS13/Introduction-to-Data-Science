{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOkGcXldr65CqyoDYCGt2Nu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SABRIS13/Introduction-to-Data-Science/blob/main/Big_Data_Pandas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color='CB1FA9'> Divide y Venceras: Big Data"
      ],
      "metadata": {
        "id": "ZmbROKogdJtU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Importando un archivo CSV gigante"
      ],
      "metadata": {
        "id": "q_VAw_Rudlrj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "“Generar un archivo CSV con únicamente los viajes que tuvieron ‘5’ estrellas”  El archivo tiene 150MM de líneas."
      ],
      "metadata": {
        "id": "fwK8XE8jdtWn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chunksize"
      ],
      "metadata": {
        "id": "3-MFVxQMdy0L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zdxqyd6dChQ"
      },
      "outputs": [],
      "source": [
        "# Definimos el tamaño máximo de filas de cada partición (1MM)\n",
        "import pandas as pd\n",
        "size = 1000000\n",
        "# Creamos un objeto iterador 'df_chunk' que contendrá las particiones con 1MM de filas en cada iteración\n",
        "df_chunk = pd.read_csv('rides.csv', chunksize=size)\n",
        "# Creamos una variable booleana verdadera 'header' para exportar las cabeceras una única vez\n",
        "header = True\n",
        "\n",
        "# Ahora vamos a recorrer cada partición, realizar el filtro solicitado, y exportar el resultado a un nuevo CSV\n",
        "# El atributo mode='a' (APPEND) sirve para no sobreescribir el archivo Resultado.csv en cada iteración, sino para siempre adjuntar los nuevos resultados\n",
        "# Luego de la 1ra iteración 'header' vuelve a ser falsa para no colocar nuevamente las cabeceras en el CSV final\n",
        "for chunk in df_chunk:\n",
        "    chunk_filter = chunk[chunk['estrellas'] == 5]\n",
        "    chunk_filter.to_csv('Resultado.csv', header=header, mode='a')\n",
        "    header = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.  Importando una tabla SQL gigante"
      ],
      "metadata": {
        "id": "I3nUxuERd41A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Exportar a TXT el contenido de una tabla SQL “Factura” que contiene toda la facturación del mes de una conocida empresa de celulares\"\n",
        "Esta tabla tiene 100MM de líneas"
      ],
      "metadata": {
        "id": "2xoj-9_0eDJ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chunksize en conexiones sql"
      ],
      "metadata": {
        "id": "HUXxBMFJeIdu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos las bibliotecas\n",
        "import pandas as pd\n",
        "import teradatasql\n",
        "\n",
        "# Configuramos los datos de conexión al banco SQL\n",
        "user = \"***\"\n",
        "password = \"***\"\n",
        "host = '***'\n",
        "query=\"SELECT * FROM FACTURA\"\n",
        "\n",
        "# Abrimos la conexión al Banco\n",
        "with teradatasql.connect(host=host, user=user, password=password, logmech='LDAP') as connect:    \n",
        "    # Creamos una variable booleana verdadera 'header' para exportar las cabeceras una única vez\n",
        "    header = True\n",
        "    # Ejecutamos la query de selección de la tabla 'Factura', pero importamos 1MM de filas de cada vez\n",
        "    # Para cada 'chunk' (DataFrame con 1MM de filas) exportamos el resultado en un nuevo TXT utilizando el modo 'APPEND'\n",
        "    for chunk in pd.read_sql(query, connect, chunksize=1000000):\n",
        "        chunk.to_csv('Factura.txt', header=header, index=None, sep='|', mode='a')\n",
        "        header = False"
      ],
      "metadata": {
        "id": "Ib3z8stTd8Dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.Importando múltiples archivos"
      ],
      "metadata": {
        "id": "_5dwDYS8eTxq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "“Subir estos 2 años de tráfico a una tabla y dividirla en archivos mensuales, donde cada archivo tendrá el tráfico de un mes”\n",
        "Tenemos más de 200 archivos y además la suma de todos ellos generará 250MM"
      ],
      "metadata": {
        "id": "LIzUp_wLebg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importando bibliotecas\n",
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Configurando el directorio donde se encuentran nuestros archivos\n",
        "path = r'D:/files'\n",
        "all_files = glob.glob(path + \"/*\")\n",
        "\n",
        "# Ahora vamos a recorrer uno a uno los archivos de nuestro directorio, y dividir cada uno en archivos mensuales\n",
        "for filename in all_files:\n",
        "    # Cada archivo será importado al DataFrame 'datos', como no son archivos grandes no usaremos el atributo 'chunksize', aunque ya sabemos como usarlo\n",
        "    datos = pd.read_csv(filename, sep = '|', header = None, dtype=str)\n",
        "    # Crearemos nombres de columnas, porque los archivos no los tienen\n",
        "    datos.columns = ['OPERATOR_ID','MONTH_DT','CUSTOMER_ID','SUBSCRIBER_ID','MSISDN_ID','IMEI_ID','IMSI_ID','TAC_ID','TRAFICO_DT','IMEI_IND','IMEI_ORIGIN_IND','USER_4P_ID','MSISDN_WITH_PREFIX_ID']\n",
        "    # Actualizaremos el valor de la columna 'MONTH_DT' (YYYYMM) con el año-mes de la fecha del tráfico que se encuentra en la columna 'TRAFICO_DT' (YYYYMMDD)\n",
        "    datos['MONTH_DT'] = datos['TRAFICO_DT'].str[0:6:1]\n",
        "    # Ahora crearemos un objeto 'grupos' que tendrá una partición(DataFrame) por cada valor de la columna 'MONTH_DT' \n",
        "    grupos = datos.groupby('MONTH_DT')\n",
        "    # Ahora que ya tenemos dividido nuestro archivo, vamos a guardar cada partición en su respectivo archivo mensual con el modo 'APPEND'\n",
        "    for mes,valores in grupos:\n",
        "        if mes=='201901':\n",
        "            valores.to_csv(r'D:\\resultados\\TRAFICO_20190100', header=False, index=None, sep='|', mode='a')\n",
        "        elif mes=='201902':\n",
        "            valores.to_csv(r'D:\\resultados\\TRAFICO_20190200', header=False, index=None, sep='|', mode='a')\n",
        "        # Completar para los demás meses del intervalo de 2 años\n",
        "        elif mes=='202012':\n",
        "            valores.to_csv(r'D:\\resultados\\TRAFICO_20201200', header=False, index=None, sep='|', mode='a')\n",
        "        elif mes=='202101':\n",
        "            valores.to_csv(r'D:\\resultados\\TRAFICO_20210100', header=False, index=None, sep='|', mode='a')\n",
        "        else:\n",
        "            valores.to_csv(r'D:\\resultados\\otros', header=False, index=None, sep='|', mode='a')"
      ],
      "metadata": {
        "id": "XDvYuTOYeWMU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}